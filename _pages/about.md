---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

My research interests encompass 1) Privacy-preserving machine learning, particularly deep learning with differential privacy, and 2) Memorization mechanisms within large-scale models. 

I am a member of the joint Ph.D. program between [Sun Yat-sen University](https://en.wikipedia.org/wiki/Sun_Yat-sen_University) and [Microsoft Research Asia](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/). My advisors are Prof. [Jian Yin](https://openreview.net/profile?id=~Jian_Yin3) and Prof. [Tie-Yan Liu](https://www.microsoft.com/en-us/research/people/tyliu/). I received my B.S. degree in Computer Science from Sun Yat-sen University in 2019. My research is supported by [Microsoft Research Asia Fellowship](https://www.microsoft.com/en-us/research/academic-program/fellowships-microsoft-research-asia/). 

Here is the [link to my resume](https://drive.google.com/file/d/1wUojmDtDDQYA2iBVYYy6r726x2zdLg6h/view?usp=sharing) (last updated on January 16, 2024).






Publications
======

[Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent](https://arxiv.org/abs/2206.02617)<br>
**Da Yu**, Gautam Kamath\*, Janardhan Kulkarni\*, Tie-Yan Liu\*, Jian Yin\*, Huishuai Zhang\*<br>
Transactions on Machine Learning Research (TMLR), 2023
    
[Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping](https://openreview.net/pdf?id=oze0clVGPeX)<br>
Jiyan He\*, Xuechen Li\*, **Da Yu**\*, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai Yu, Jiang Bian<br>
International Conference on Learning Representations (ICLR), 2023

[Differentially Private Fine-tuning of Language Models](https://arxiv.org/abs/2110.06500), [[code]](https://github.com/huseyinatahaninan/Differentially-Private-Fine-tuning-of-Language-Models)<br>
**Da Yu**, Saurabh Naik, Arturs Backurs\*, Sivakanth Gopi\*, Huseyin A. Inan\*, Gautam Kamath\*, Janardhan Kulkarni\*, Yin Tat Lee\*, Andre Manoel\*, Lukas Wutschitz\*, Sergey Yekhanin\*, Huishuai Zhang\*<br>
International Conference on Learning Representations (ICLR), 2022

[Large Scale Private Learning via Low-rank Reparametrization](https://arxiv.org/abs/2106.09352), [[code]](https://github.com/dayu11/Differentially-Private-Deep-Learning)<br>
**Da Yu**, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu<br>
International Conference on Machine Learning (ICML), 2021

[Do not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning](https://arxiv.org/abs/2102.12677), [[code]](https://github.com/dayu11/Differentially-Private-Deep-Learning/tree/main/vision/GEP)<br>
**Da Yu**\*, Huishuai Zhang\*, Wei Chen, Tie-Yan Liu<br>
International Conference on Learning Representations (ICLR), 2021

[Availability Attacks Create Shortcuts](https://arxiv.org/abs/2111.00898), [[code]](https://github.com/dayu11/Availability-Attacks-Create-Shortcuts)<br>
**Da Yu**, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu<br>
SIGKDD Conference on Knowledge Discovery and Data Mining, Research Track (KDD), 2022

[How Does Data Augmentation Affect Privacy in Machine Learning?](https://arxiv.org/abs/2007.10567), [[code]](https://github.com/dayu11/MI_with_DA)<br>
**Da Yu**, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu<br>
AAAI Conference on Artificial Intelligence (AAAI), 2021

[Stabilize Deep ResNet with A Sharp Scaling Factor $\tau$](https://arxiv.org/abs/1903.07120), [[code]](https://github.com/dayu11/tau-ResNet)<br>
Huishuai Zhang, **Da Yu**, Mingyang Yi, Wei Chen, Tie-Yan Liu<br> 
Machine Learning, 2022


[Gradient Perturbation is Underrated for Differentially Private Convex Optimization](https://arxiv.org/abs/1911.11363)<br>
**Da Yu**, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu<br>
International Joint Conference on Artificial Intelligence (IJCAI), 2020

Manuscripts
======
[Privacy-Preserving Instructions for Aligning Large Language Models](https://www.researchgate.net/profile/Da-Yu-14/publication/378342349_Privacy-Preserving_Instructions_for_Aligning_Large_Language_Models/links/65d5ac58c3b52a1170e98f84/Privacy-Preserving-Instructions-for-Aligning-Large-Language-Models.pdf?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6ImxvZ2luIiwicGFnZSI6InB1YmxpY2F0aW9uIn19)<br>
**Da Yu**, Peter Kairouz\*, Sewoong Oh\*, Zheng Xu\*<br>
Preprint, 2024


[Selective Pre-training for Private Fine-tuning](https://arxiv.org/abs/2305.13865)<br>
**Da Yu**, Sivakanth Gopi, Janardhan  Kulkarni, Zinan Lin, Saurabh Naik, Tomasz Lukasz Religa, Jian Yin, Huishuai Zhang<br>
Preprint, 2023

[Challenges towards the Next Frontier in Privacy](https://arxiv.org/abs/2304.06929)<br>
Rachel Cummings, Damien Desfontaines, David Evans, Roxana Geambasu, Matthew Jagielski, Yangsibo Huang, Peter Kairouz, Gautam Kamath, Sewoong Oh, Olga Ohrimenko, Nicolas Papernot, Ryan Rogers, Milan Shen, Shuang Song, Weijie Su, Andreas Terzis, Abhradeep Thakurta, Sergei Vassilvitskii, Yu-Xiang Wang, Li Xiong, Sergey Yekhanin, **Da Yu**, Huanyu Zhang, Wanrong Zhang<br>
Preprint, 2023

Academic Service
======
I am a reviewer for ICML 2022-2024, NeurIPS 2022-2023, and ICLR 2022-2024. I'm awarded as a top reviewer for several times.

